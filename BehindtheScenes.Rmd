---
title: "Applying Machine Learning to Bitcoin Price Prediction"
author: "Tshering Wangchuk & Aaron Salot"
date: 12/08/2021
output: 
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    code_download: true
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(readxl)
library(caret)
library(tidyverse)         # for graphing and data cleaning
library(tidymodels)        # for modeling
library(themis)            # for step functions for unbalanced data
library(doParallel)        # for parallel processing
library(stacks)            # for stacking models
library(naniar)            # for examining missing values (NAs)
library(lubridate)         # for date manipulation
library(moderndive)        # for King County housing data
library(vip)               # for variable importance plots
library(patchwork)         # for combining plots nicely
library(DALEX)
library(DALEXtra)
library(rpart)
library(ggthemes)
theme_set(theme_minimal()) # Lisa's favorite theme
```

### Introduction

<br> Bitcoin, and other cryptocurrencies, have been present for over a decade, and have become increasingly prevalent in the global economic system. Bitcoin, the first and most widely used cryptocurrency, is used as a store of value, tradable asset, and a form of currency. This analysis aims to take advantage of the open-source blockchain network behind Bitcoin, and our goal is to understand the various factors that influence the price movement of Bitcoin. 


<br> Bitcoin popularized blockchain technology, and Satoshi Nakamoto used it to solve the Byzanztine's General Problem and the double-spending problem without the need of a central authority. It is also the fastest growing asset class to have reached close to $2-3 trillion in market capitalization in over a decade. Given the recent technological adoption as seen with Tesla, Microstrategy & El Salvador's government, we are particularly interested to examine the price movement of Bitcoin by looking at certain variables that are publically accessible through Bitcoin's public ledger.


### Data Sources

<br> We used three different data sources: CoinMetrics, Glassnodes & FRED. We collected an overall of 162 variables with over 4000 observations; however, after basic explanatory analysis and cleaning, we ended up with 88 variables and 2,100 observations. CoinMetrics was the largest bulk of our data, where we used their public API to extract daily data from 2010 to present (November, 2021). We also secured a Glassnodes membership that gave access to daily changes in select variables not present in the CoinMetrics API. Taking from the previous literature review, we also decided to incorporate certain variables that have been examined in previous papers - CPI, Gold Price, Nasdaq & Bitcoin Google Search trends. 


### Loading the Data

We started with 11 years of data with 4000+ observations, and decided to cut our dataset to the most recent 5-years due to data-discrepancy and since Bitcoin was in its nascent stages earlier. 

```{r}
ADS_Project_11_29_2021_v8 <- read_excel("ADS_Project - 11.29.2021.v8.xlsx")
```

### Data Preprocessing 

 We filtered through the variables and removed repetitive ones. For example, certain variables were displayed in USD and native units - we removed variables in native units to ensure consistency and standardization throughout. We also removed that overlap - `NVT, adjusted, 90d MA` & `NVT, free float, 90d MA` are examples of two that we removed to prevent over fitting. 

```{r}
Master <- ADS_Project_11_29_2021_v8 %>%
select(-c(`Last Calculated EOD metrics (time, in seconds)`, `Block Count`, `Block Size (mean, in bytes)`, `Block Weight, (mean`, `Capitalization of current supply, MVRV`, `Capitalization of free float, MVRV`, `Difficulty, Last Block`, `Difficulty, Mean Block`, `Mean Transaction Fee per Byte of all blocks, native units`, `Mean Transaction Fee per Byte, native units`, `Median Transaction Fee, native units`, `Median Fee per Transacation, USD`, `Addresses balance greater than 0.001 native units`, `Addresses balance greater than 0.01 native units`, `Addresses balance greater than 0.1 native units`,`Addresses balance greater than 100 native units`, `Addresses balance greater than 100K native units`, `Addresses balance greater than 10 native units`, `Addresses balance greater than 10K native units`, `Addresses balance greater than 1 native units`, `Addresses balance greater than 1K native units`, `Addresses balance greater than 1M native units`, `Addresses balance greater than 1in100K`, `Addresses balance greater than 1in100M`, `Addresses balance greater than 1in10B`, `Addresses balance greater than 1in10K`, `Addresses balance greater than 1in10K`, `Addresses balance greater than 1in10M`, `Addresses balance greater than 1in1B`, `Addresses balance greater than 1in1K`, `Addressesbalance greater than 1in1M`, `Total Fees, native units`, `Flow to exchanges, native units`, `Flow out exchanges, native units`, `Gas Limit Block`, `Gas Limit, transaction`, `Gas Used, transaction`, `Hash Rate Mean, 30d`, `Issuance, native units`, `Issuance, percent annualized`, `Issuance,  percent daily`, `Issuance Total, native units`,`NVT, adjusted, 90d MA`, `NVT, adjusted, free float,  90d MA`, `ROI, 1y (%)`, `Revenue, per hash unit (Native Currency)`, `Revenue Daily, per hash unit/second (Native Currency)`, `Miner Rev (Native Currency)`, `Supply, 10y Active`, `Supply, 5yr Active`, `Supply, 4yr Active`, `Supply, 3yr Active`, `Supply, 1yr Active Trailing (%)`, `Supply, in addresses (<0.001, native currency)`, `Supply, in addresses (<0.01M, native currency)`, `Supply, in addresses (<0.1, native currency)`, `Supply, in addresses (<1, native currency)`, `Supply, in addresses (<10, native currency)`, `Supply, in addresses (<100, native currency)`, `Supply, in addresses (<100K, native currency)`, `Supply, in addresses (<10K, native currency)`, `Supply, in addresses (<1K, native currency)`, `Supply, in addresses (<1M, native currency)`, `Supply, Miner address with 1 mining entity (native currency)`, `Supply, All Miners (native currency)`, `Transactions, adjusted transfer value (native currency)`, `Transactions, mean transfer value (native currency)`, `Transactions, median transfer value (native currency)`, `Transactions, mean transfer value ($)`, `Transactions, median transfer value (native currency)`, `Velocity, 1yr current supply`, `Volatility, 180d daily returns`, `Volatility, 30d daily returns`, `Price, BTC`, `Price, ($)`, `Transactions, median transfer value ($)`))
```

We arrived at 88 variables and 2145 observations. We then plotted over 60 graphs (all attached in the behind-the-scenes) and removed certain variables that either did not have any correlation to the output or where other variables had a visually overlapping effect. Below are plots of select variables that we removed during this process. 

```{r}
ggplot(Master,
       aes(x = `Hash Rate, mean`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) + theme_economist()

ggplot(Master,
       aes(x = `Issuance, in USD`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) + theme_economist()

ggplot(Master,
       aes(x = `Issuance Total, in USD`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) + theme_economist()


ggplot(Master,
       aes(x = `Supply, 30D Active`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) + theme_economist()

ggplot(Master,
       aes(x = `Supply, 7D Active`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) + theme_economist()

ggplot(Master,
       aes(x = `Supply, 90D Active`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE)+ theme_economist()
```


<br> Consequently, the graphical explandatory data analysis showed certain variables that demonstrated a strong corelation to the output. Some of these variables can be seen below. 

```{r, warning=FALSE}
ggplot(Master,
       aes(x = `Gas Limit Block, mean`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) + theme_economist()

ggplot(Master,
       aes(x = `NVT, adjusted`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) +
   theme_economist()

ggplot(Master,
       aes(x = `Supply, in addresses (<$10M)`,
           y = `Price`)) + 
         geom_point() + 
         geom_smooth(method = lm, se = FALSE) +
   theme_economist()
```

The code below shows the variables we removed after plotting most of the graphs as seen above. This further dropped our variable count from 88 variables to 27. 

```{r}
Master <- Master %>% select(-c(`Gas limit Mean, transaction`, `Hash Rate, mean`, `Issuance, in USD`, `Issuance Total, in USD`, `ROI, 30D (%)`, `Revenue Daily, per hash unit/second (USD)`, `Revenue, per hash unit ($)`, `Supply Equality Ratio`, `Supply, 180D Active`, `Supply, 1yr Active`, `Supply, 2y Active`, `Supply, 1yr Active`, `Supply, Miner address with 1 mining entity ($)`, `Transactions, count`,`Transactions, count (per second)`, `Transactions, adjusted transfer value ($)`, `Coin_Days_Destroyed`, `Balance_Exchanges_(%)`, `Total_Exchange_Inflow_Volume`, `Supply, 30D Active`, `Supply, 7D Active`, `Supply, 90D Active`, `Supply, address balance (<1in10K)`, `Supply, address balance (<1in10M)`, `Supply, address balance (<1in10B)`, `Supply, address balance (<1in10K)`, `Supply, address balance (<1in1B)`, `Supply, address balance (<1in1K)`, `Supply, address balance (<1in1M)`, `Supply, in addresses (<$1)`, `Supply, in addresses (<$10)`, `Supply, in addresses (<$100)`, `Supply, in addresses (<$100K)`, `Supply, in addresses (<$10K)`, `Supply, in addresses (<$10M)`, `Supply, in addresses (<$1K)`, `Supply, held by top 1% addresses`, `Supply, 10yr expected supply`, `Supply, free float`, `Supply, All Miners ($)`, `Supply, Miner address with 1 mining entity ($)`, `Transactions, count`, `Transactions, adjusted transfer value ($)`, `Coin_Days_Destroyed`, `CPI_(%)`))
```

```{r}
Master <- Master %>% select(-c(`All Addresses balance`, `Addresses balance greater than $100`, `Addresses balance greater than $10`, `Addresses balance greater than $1`, `Capitalization of active native units supply, in USD`, `Capitalization of free float, in USD`, `Capitalization realized, USD`, `NVT, adjusted, free float`, `Miner Rev ($)`, `Supply, current`, `Flow transfers exchanges`, `Supply, address balance (<1in100M)`, `Supply, in addresses (<$1M)`, `Transactions, transfer count`, `NUPL`, `rHODL`, `Addresses Active Count`,`Addresses balance greater than $10K`, `Addresses balance greater than $100K`, `Addresses balance greater than $1K`, `Addresses balance greater than $1M`, `Block, weight, total`))
```


In the section below, we carry out further pre-processing. We start by converting the Date into numeric format from character, so this will not cause problems when building the recipes for the models moving forward. We also introduce a few new variables which represent the lagged characteristics of the Prices such as *lag1day* which essentially enables each observation to obtain more than 1 price variable; hence the *Price* variable will represent that specific days price and *lag1day* will represent the previous day's price.

```{r}
 Master <- Master %>%
           mutate(Date = ymd(Date)) %>% 
           mutate(Date_num = as.numeric(Date)) %>%
           mutate(lag1day = lag(Price, 1, order_by = Date),
                  lag7day = lag(Price, 7, order_by = Date)) %>%
           replace_na(list(lag1day = 0, lag7day = 0)) %>% 
           mutate(Daily_avg = (Price + lag1day)/2,
                  Weekly_avg = (Price + lag7day)/7) %>%
           select(-c(CPI, `BTC(%)`, `NDQ(%)`, `Gold(%)`, BTC_Searches, `BTCS(%)`))
```

*** 

### Machine Learning Models 

We make use of three core models in our analysis --- 
              + Lasso Classification
              + Random Forest
              + Decision Trees

We also carry out further exploratory analysis with Support Vector Machines to see how it performs

#### Lasso - Binary Classification 

Initially, we wanted to use a lasso predictive regression which would model a quantitative *Y* variable (*Price*). Upon preliminary model building and testing, we observed that the model performed very poorly with this approach. This was likely due to Bitcoin having extremely volatile prices creating increased variation and noise which made modelling it accurately difficult. Hence, we moved forward with a binary lasso classification.

In building the Lasso, we created a new variable called *PriceDirection* which would allow us to look at price actions in a binary structure with the help of the lagged variables. We define the main dependent variable by passing on a logical condition whereby if *Price* is greater than *lag1day*, then the condition is true and represents a price increase (1). If the condition fails, it will translate to a price decrease (0). This is shown below. 

```{r}
Master <- Master %>% mutate(Date = ymd(Date)) %>% 
           mutate(Date_num = as.numeric(Date)) %>%
           mutate(PriceDirection = ifelse(Price > lag1day, yes = 1, no = 0)) %>%
           mutate(PriceDirection = as.factor(PriceDirection)) 
        
```


We take a proportional split of 3/4 for the split between Train and Test, whereby 0.75 is attributed to the training set and 0.25 is used for testing. Given the rapid acceleration in the price of Bitcoin over the years, we decided not to use a time split. 

```{r}
set.seed(456)
Master_Split <- initial_split(Master, prop = 0.75)

Master_Training <- training(Master_Split)
Master_Testing <- testing(Master_Split)
```


In the next step, we prepare the Lasso model recipe. We remove *Date* since the recipe building can only process numeric data structures. We also exclude *Network Distribution Factor*, *Capitalization of Current Supply, in USD*, and *Supply, held by top 10% addresses* as these variables demonstrated issues of Endogeneity. We also exclude other variables which are not used for this section.

```{r}
lasso_recipe <- recipe(PriceDirection ~ .,
                       data = Master_Training) %>%
                step_rm(Date, Weekly_avg, Daily_avg, lag7day, lag1day, `Network distribution factor`, `Capitalization of Current Supply, in USD`, `Supply, held by top 10% addresses`) %>%
                step_mutate_at(all_numeric(), -all_outcomes(), fn= ~ as.numeric(.)) %>%
                step_dummy(all_nominal(), -all_outcomes()) %>%
                update_role(Date_num, new_role = "ID") %>%
                step_zv(all_predictors()) %>%
                step_normalize(all_predictors(), -all_nominal())

lasso_recipe %>% prep() %>% juice()
```

```{r}
lasso_mod <- logistic_reg(mixture = 1) %>%
             set_engine("glmnet") %>% 
             set_args(penalty = tune()) %>% 
             set_mode("classification")

lasso_wf <- workflow() %>%
            add_recipe(lasso_recipe) %>%
            add_model(lasso_mod) 
```

```{r}
set.seed(494)
btc_cv <- vfold_cv(Master_Training,
                   v = 10)

penalty_grid <- grid_regular(penalty(),
                             levels = 10)

ctrl_grid <- control_stack_resamples()

metric <- metric_set(accuracy)
```

```{r}
lasso_tune <- lasso_wf %>%
              tune_grid(resamples = btc_cv,
                        grid = penalty_grid,
                        control = ctrl_grid)

lasso_tune %>% collect_metrics()
```

```{r}
best_param <- lasso_tune %>%
              select_best(metric = "accuracy")
best_param
```

```{r}
final_lasso <- lasso_wf %>% finalize_workflow(best_param) %>%
               fit(data = Master_Training)

final_lasso %>% pull_workflow_fit() %>% tidy()
```

```{r}
library(vip)
final_lasso %>% 
      fit(Master_Training) %>%
      extract_fit_parsnip() %>%
      vi(lamda = lowest_rmse$penalty) %>%
      mutate(Importance = abs(Importance),
             Variable = fct_reorder(Variable, Importance)) %>%
     ggplot(aes(x = Importance, y = Variable, fill = Sign)) +
     geom_col() + 
     scale_x_continuous(expand = c(0,0)) +
     labs(y = NULL)
```

```{r}
lasso_fit <- final_lasso %>%
  last_fit(Master_Split)

lasso_fit

test_performance_lasso <- lasso_fit %>% collect_metrics()
test_performance_lasso
```

The Lasso classification model selects the variables based in the magnitude of their coefficients. We built the Lasso by using a penalty grid level of 10, CV resampling of 10 folds, and moved forward by selecting the tuning parameter with the highest prediction accuracy as the parameter for the final model. As shown above, the prediction accuracy for the lasso model is 59.4%, predicting the correct price movement 59.4% of the time with Receiver operating characteristic area under curve of 62%.   

#### Random Forest - Classification

The next model we use is a Random Forest. Similar to the lasso model, we take most of the same concepts and ideas in building the model with *PriceDirection* being the main dependent variable. We adjust for the removal of the variables we excluded in the lasso for the same reasons as above. 

We build our random forest model using 10 trees with penalty grid of level 3 to tune the forest and a resampling CV of 10 folds. The model returns the the mode of the predictions for the 10 trees. 

```{r}
set.seed(456)
rf_recipe <- recipe(PriceDirection ~.,
                    data = Master_Training) %>%
             step_mutate_at(all_numeric(), fn = ~as.numeric(.)) %>%
             step_rm(Weekly_avg, Daily_avg, lag7day, lag1day, `Network distribution factor`, `Capitalization of Current Supply, in USD`, `Supply, held by top 10% addresses`) 

rf_recipe %>% prep() %>% juice()
```

```{r}
rf_model <- rand_forest(mtry = tune(),
                        min_n = tune(),
                        trees = 10) %>%
            set_mode("classification") %>%
            set_engine("ranger")

rf_workflow <- workflow() %>%
               add_recipe(rf_recipe) %>%
               add_model(rf_model)

rf_penalty_grid <- grid_regular(finalize(mtry(),
                                Master_Training %>%
                                select(-PriceDirection)),
                                min_n(),
                                levels = 2)

rf_tune <- rf_workflow %>%
           tune_grid(resamples = btc_cv,
                     grid = rf_penalty_grid,
                     control = control_stack_grid())

rf_tune %>% collect_metrics()

```

```{r}
best_param_rf <- rf_tune %>%
              select_best(metric = "accuracy")
best_param_rf
```
```{r}
rf_final <- rf_workflow %>% finalize_workflow(best_param_rf)
rf_fit <- rf_final %>%
  last_fit(Master_Split)

rf_fit

test_performance_rf <- rf_fit %>% collect_metrics()
test_performance_rf
```

We observe that the random forest performs poorly compared to the lasso model. Our best performing random forest model with the best tuned parameter has a prediction accuracy of 52.3% when tested against the test split. The highest prediction accuracy is seen for tuning parameter mtry = 26 and min_n = 2 with a prediction accuracy of 56.3%. 

#### Decision tree 

The last model we use to create a stacked model is a decision tree. We utilize the same recipe built for the random forest to build the tree model with the same parameters. The results we obtained below were suprising because the tree model outperforms the random forest model with model prediction accuracy of 59.8% and test accuracy of 60%. The tree model delivers the highest prediction accuracy for testing among all the three models. 

```{r}
#decision trees
set.seed(456)

tree_model <-
  decision_tree() %>%
  set_mode("classification") %>%
  set_engine("rpart")

tree_workflow <-
  workflow() %>%
  add_recipe(rf_recipe) %>%  
  add_model(tree_model)

tree_fit <-
  tree_workflow %>%
  fit_resamples(btc_cv,
                control = control_stack_resamples()
  )

collect_metrics(tree_fit)
```

```{r}
best_param_tree <- tree_fit %>%
              select_best(metric = "accuracy")
best_param_tree
```

```{r}
tree_final <- tree_workflow %>% finalize_workflow(best_param_rf)

Tree_fit <- tree_final %>%
  last_fit(Master_Split)

Tree_fit

test_performance_tree <- Tree_fit %>% collect_metrics()
test_performance_tree
```


```{r, echo=FALSE}
library(vip)
tree_final %>% 
      fit(Master_Training) %>%
      extract_fit_parsnip() %>%
      vi(accuracy = best_param_tree) %>%
      mutate(Importance = abs(Importance),
             Variable = fct_reorder(Variable, Importance)) %>%
     ggplot(aes(x = Importance, y = Variable)) +
     geom_col() + 
     scale_x_continuous(expand = c(0,0)) +
     labs(y = NULL)
```

#### Stacking 

```{r}
lasso_tune %>%
  collect_metrics()
```

```{r}
rf_tune %>%
  collect_metrics()
```

```{r}
tree_fit %>%
  collect_metrics()
```

```{r, warning=FALSE}
Models_stack <-
  stacks() %>%
  add_candidates(lasso_tune) %>%
  add_candidates(rf_tune) %>%
  add_candidates(tree_fit)
```

```{r}
Models_blend <-
  Models_stack %>%
  blend_predictions()
Models_blend
```

```{r, warning=FALSE}
Model_final_stack <- Models_blend %>%
  fit_members() 
```

### Data Visualization

```{r}
Master %>% 
  select(`Price`, `Date_num`, `SOPR`, `Adjusted_Dormancy_Flow`) %>%
  pivot_longer(cols=-`Date_num`, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(`Date_num`, `values`)) + geom_line() + facet_wrap(vars(`variable`), ncol = 1, scales = "free_y") + theme_wsj()
```

This plot shows the dormancy flows of dormant coins throughout a bitcoin cycle. For example, at the peak of the cycle in 2018 (Date_num < 17500), most of the the dormant coins were being trasacted and tranferred across changes. It comes to show how in the current cycle the dormant coins have not reached that level of transactions and movement. The SOPR, on the other hands, helps understand short-term price movement. During large dumps, like March 2020 (Date_num < 18500), we see how the SOPR drops significantly around 0.9, while it reaches closer to 1.10 at bull market local peaks. 


```{r}
Master %>% 
  select(`Price`, `Date_num`, `Supply, held by top 100 addresses`, `Supply, Total Active`) %>%
  pivot_longer(cols=-`Date_num`, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(`Date_num`, `values`)) + geom_line() + facet_wrap(vars(`variable`), ncol = 1, scales = "free_y") + theme_wsj()
```

One of the interesting variables to examine is Supply, held by the top 100 addresses. These are the largest bitcoin holders, and their holdings often dictate the price direction of Bitcoin - this is primarily due to the fact that they have experienced multiple cycles, can anticipate how the current cycle may play out, and potentially because they hold greater information about upcoming price catalysts. Them increasing their holdings significantly during the March 2020 dump could be a potential indicator to frontrun a large increase in the price of bitcoin. 

```{r}
Master %>% 
  select(`Price`, `Date_num`, `Coin_Years_Destroyed`, `NVT, adjusted`) %>%
  pivot_longer(cols=-`Date_num`, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(`Date_num`, `values`)) + geom_line() + facet_wrap(vars(`variable`), ncol = 1, scales = "free_y") + theme_wsj()
```

The Network Value to Transaction helps understand the relationship between transfer volume and the market cap of Bitcoin. In order words, a higher NVT indicates that the valuation of the Bitcoin network is greater than the transaction volume (a more intrinsic value indicator). The blow-off top in 2018 could be anticipated by the sudden increase in NVT, which would indicate a bubble-like behavior in terms of Bitcoin's transaction volume vs it's market cap. 

Below are additional graphs plotted with the price of Bitcoin over the span of 5 years. 

```{r}
Master %>% 
    select(`Price`, `Date_num`,`Miner Rev, Total ($)`, `Total Fees, USD`) %>%
  pivot_longer(cols=-`Date_num`, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(`Date_num`, `values`)) + geom_line() + facet_wrap(vars(`variable`), ncol = 1, scales = "free_y") + theme_wsj()
```

```{r}
Master %>% 
  select(`Price`, `Date_num`, `Gas Limit Block, mean`, `Addresses balance greater than $10M`) %>%
  pivot_longer(cols=-`Date_num`, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(`Date_num`, `values`)) + geom_line() + facet_wrap(vars(`variable`), ncol = 1, scales = "free_y") + theme_wsj()
```

```{r}
Master %>% 
    select(`Price`, `Date_num`,`Flow out exchanges, USD`, `Mean Fee per Transaction, USD`) %>%
  pivot_longer(cols=-`Date_num`, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(`Date_num`, `values`)) + geom_line() + facet_wrap(vars(`variable`), ncol = 1, scales = "free_y") + theme_wsj()
```

### Data Dictionary

Spent Output Profit Ratio (SOPR): Price Paid / Price Sold
Gas Limit Block, Mean: Sum gas limit of all block from that interval
Adjusted Dormancy Flow: Market Cap/Annualized Dormancy Value
Coin Years Active: Coins Transacted / # of Days Since Last Transacted
NVT, Adjust: Market Cap/Transfer Volume


### Supplementary Model

#### SVM - Classification 

We also carry out supplementary analysis with Support Vector Machines because of their advantages in classification tasks. We observe that for the main Support vector machine model we built, the prediction accuracy does not differ by a considerable rate from the three models we have used above. 

```{r}
Master_2 <- Master %>%
            mutate("PriceDirection" = factor(ifelse(PriceDirection == "1" | PriceDirection != "0",
                                                    "Price_Increase", "Price_Decrease"),
                                                    levels = c("Price_Increase", "Price_Decrease")))
```

```{r}
Master_Split2 <- initial_split(Master_2, prop = 0.75)

Master_Training2 <- training(Master_Split2)
Master_Testing2 <- testing(Master_Split2)
```

```{r}
btc_cv2 <- vfold_cv(Master_Training2,
                   v = 10)
```

```{r}
svm_rec <- 
  recipe(PriceDirection ~ ., data = Master_2) 

svm_spec <-
  svm_rbf(cost = tune(), rbf_sigma = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

```{r}
svm_wflow <- 
  workflow() %>% 
  add_model(svm_spec) %>% 
  add_recipe(svm_rec)
```

```{r}
cost()
rbf_sigma()
```

```{r}
svm_param <- 
  svm_wflow %>% 
  parameters() %>% 
  update(rbf_sigma = rbf_sigma(c(-7, -1)))

start_grid <- 
  svm_param %>% 
  update(
    cost = cost(c(-6, 1)),
    rbf_sigma = rbf_sigma(c(-6, -4))) %>% 
  grid_regular(levels = 2)

set.seed(456)

svm_initial <- 
  svm_wflow %>% 
  tune_grid(resamples = btc_cv2, 
            grid = start_grid, 
            control = control_stack_grid())
```

```{r}
collect_metrics(svm_initial)
```


```{r}
best_param_svm <- svm_initial %>%
              select_best(metric = "accuracy")
best_param_svm
```

##### Bayesian Optimization for SVM 

```{r}
ctrl <- control_bayes(verbose = TRUE)

set.seed(420)

svm_bo <-
  svm_wflow %>%
  tune_bayes(
    resamples = btc_cv2, 
    metrics = NULL, 
    initial = svm_initial, # tune_grid object produced earlier
    param_info = svm_param, # specified earlier too, with our new bounds for rbf_sigma
    iter = 27, # maximum number of search iterations
    control = ctrl
  )
```

```{r}
show_best(svm_bo)
```

```{r}
svm_bo %>% collect_metrics()
```





